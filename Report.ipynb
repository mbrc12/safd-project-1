{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAFD1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOuH1DYZbSEg"
      },
      "source": [
        "# Regression in terms of Trees and Forests\n",
        "\n",
        "### **Group 3:** _Ashish Agarwal, Bagal Satej Babanrao, Kushagra Mahajan, Mihir Yadav, Mriganka Basu Roy Chowdhury_\n",
        "\n",
        "Over the last few decades, the statistical learning/inference community has experienced a boom. With the advent of AI/ML, people have increasingly turned to sophisticated methods like Deep Learning and Reinforcement Learning. However, for a wide range of tasks of practical use, \"classical\" algorithms still fare sufficiently well. And what is more important -- the theory of these classical estimators is fairly well understood. We now know how they work, why they work, and how they can be modified effectively and deterministically for various use-cases. This makes their study not only relevant from a historical perspective, but also from a scientific perspective because they form that part of a data-scientists toolbox where he/she is aware of the underlying theory (as opposed to DL, which is still mostly a black-box), and consequently can wield them to considerable effect.\n",
        "\n",
        "The most popular and well studied of these classical estimators/classifiers is perhaps Regression (and its many variants). In this article we delve deeper into a form of Regression that uses Decision Trees, an extremely powerful idea in Classical ML. Towards the end of this article we will also take a look at simple ensemble methods, like the Random Forest.\n",
        "\n",
        "\n",
        "\n",
        "## Decision Trees\n",
        "\n",
        "What are decision trees? Perhaps we are better off answering this with a picture:\n",
        "\n",
        "![](https://github.com/mbrc12/random-images/raw/master/decisionTree.png)\n",
        "\n",
        "We think of $A, B, C$ as attributes of a data-point. Each node represents a question (say $A < B$ ?,  $A + 2B > 9$ ? etc) which has TRUE/FALSE answer. The two edges out of the node represent these two answers. Each leaf will essentially represent a set of inequalities. This means that a leaf node corresponds to the region (in the domain) defined by these inequalities; like the following figure:\n",
        "\n",
        "![](https://github.com/mbrc12/random-images/raw/master/splitting.png)\n",
        "\n",
        "### Regression\n",
        "\n",
        "The decision tree will split the dataset into many parts, by determining optimal attributes and corresponding splitting values. Given a new datapoint, we can now figure out to which leaf this point belongs and report the average of the decision values of all the datapoints belonging to this leaf.\n",
        "\n",
        "Why are decision trees better than simple regression? Well consider the following data-points:\n",
        "\n",
        "![](https://github.com/mbrc12/random-images/raw/master/comparison_with_linear_regression.png)\n",
        "\n",
        "As we can see, there are different regions with different characteristics. If we used plain linear regression, it would attempt to treat these differing regions as the same, thereby causing a large error (on both test and training data), simply because it is not capable of capturing this feature of a dataset. Decision trees however can divide the plane into regions and fit different structures on different parts, thereby giving a much better accuracy. This theoretical prediction is indeed also carried over in practice. \n",
        "\n",
        "## Constructing the Decision Tree\n",
        "The central idea lies in how we split a given node. We iterate over all the features and calculate the splitting value and impurity corresponding to that feature and **select the one providing the minimum impurity**, which is defined as the <b>weighted sum of the sample variances of the two child nodes</b>. This can be calculated by first sorting all the datapoints present in that node based on the corresponding feature values and then making a linear traversal.\n",
        "\n",
        "Initially we would place all the points in the right child. At every step, we would move the current datapoint from the right node to the left node and make the required changes in the sample variances of the children nodes.\n",
        "\n",
        "So how can we actually construct such a tree?\n",
        "The crux of the algorithm lies in coming up with the best splits at each node. The splits are identified with the sole purpose of minimizing the impurity at the nodes.\n",
        "\n",
        "Now how to define this 'impurity' given a set of data points at a node?\n",
        "\n",
        "For regression purposes variance of the data has proved to be a good measure of impurity. For our use case we find the split that minimizes the weighted sum of variance at each child node generated.\n",
        "\n",
        "At each step we choose a leaf $v_{i}$ and for each attribute $d_{i}$ we calculate a constant $c_{i}$ which is the split value that results in minimum impurity if the split is made on this attribute.\n",
        "This means that if a node is split at attribute $d_{i}$ with the split value $c_{i}$, the node will be split into leaves one with values $x_{d_{i}} \\le c_{i}$ and the other with $x_{d_{i}} > c_{i}$.\n",
        "\n",
        "At this point we make a core observation: \n",
        "<b>An optimal $c$ for any attribute $d$ can be chosen to be one of the $(x_{i})_{d} $</b>.\n",
        "\n",
        "Why does this hold? <br>\n",
        "Suppose the values of $(x_{i})_{k}$ are sorted as $z_{1}\\le z_{2}\\le z_{3} \\le \\cdots \\le z_{N}$ where $N$ is the number of datapoints.\n",
        "For a split value $c$ such that $z_{i} \\le c < z_{i+1}$ we have the following: all the values $\\le z_{i}$ belong to the left child and the rest to the right child. But this split remains same for any value of $c$ in $[z_{i}, z_{i+1})$. So we can just take $c$ to be $z_{i}$ itself.\n",
        "\n",
        "This makes life a bit simpler as for calculating the optimal split value for some attribute $d_{i}$ we just need to iterate over all $z_{i}$ and pick the value that results in the most variance reduction.\n",
        "\n",
        "Now the only thing that remains is to choose an attribute to split on. This happens to be trivial as we just need to iterate over all attributes and choose the one that reduces the variance most. The optimal split for each attribute is calculated by following the steps explained above.\n",
        "\n",
        "Okay, so far we have seen how we can grow our decision tree starting from a single root node by splitting the nodes at each iterative step. How long do we keep doing this?<br>\n",
        "Should we do this till we reach pure splits and cannot split further? Then we would get $100\\%$ accuracy on the training data but will most definitely end up just learning all the training data and our model will not generalize well to unseen data.\n",
        "\n",
        "It looks like we need some stopping criterion to prevent our model from overfitting.\n",
        "\n",
        "At this point let us introduce the $\\mathrm{coefficient\\ of\\ variance}$: \n",
        "\n",
        "$\\mathrm{CoefVar} = \\dfrac{\\mathrm{Standard\\ Deviation}}{\\mathrm{Mean}} = \\dfrac{\\sqrt{\\dfrac{1}{n}\\sum_{j}(y_{j}-\\bar y)^{2}}}{\\bar y}$\n",
        "\n",
        "which is a measure of the variability of data relative to the mean.\n",
        "\n",
        "At every node we calculate this coefficient and if it falls below a certain threshold, $i.e.$ the variability of data has reduced enough, the node is not split any more. \n",
        "\n",
        "\n",
        "## Code Walkthrough\n",
        "We begin with a single node, and iterate over all possible attributes and threshold values ( Function _build_ beginning from _line_ _140_ _below_) and pick the split leading to the minimum impurity. Function _get_impurity_ (_line_ _65_) computes the impurity for a given attribute. Function _get_decision_value_for_feature_ (_line_ _23_) computes the threshold splitting value correspoding to a feature. Once these are evaluated, _divide_data_ (_line_ _98_) is called to make the split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V7TeE4HsHaI"
      },
      "source": [
        "##Pruning\n",
        "\n",
        "Pruning is yet another technique to prevent overfitting of decision trees. It reduces the size of decision trees by removing parts of the tree that do not provide enough power (as opposed to the time/overfitting cost) to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this possibility.\n",
        "As the name implies, pruning involves cutting back the tree. After a tree has been built (and in the absence of early stopping discussed below) it may be overfitted. The recusive algorithm will then repeatedly partition data into smaller and smaller subsets until those final subsets are homogeneous in terms of the outcome variable. In practice this often means that the final subsets (known as the leaves of the tree) each consist of only one or a few data points. The tree has learned the data exactly, but a new data point that differs very slightly might not be predicted well. This is why pruning is an important technique, in both theory and practice.\n",
        "\n",
        "###Code Walkthrough\n",
        "The code for pruning is implemented below in function `prune_decision_tree` *(line 161 - 183 below)*\n",
        "\n",
        "* Consider an internal (non-leaf) node $v$, and let $S_v$ be the sample variance of $y_i$ of all datapoints contained in this node $v$. *(line 170 - 172)*\n",
        "* Let $S_v'$ be the sum of the sample variances of $y_i$ of all leaves which are descendants of $v$. *(line 174)*\n",
        "* Clearly $S_v' < S_v$, because splitting \"improves\" the sample variance.\n",
        "* But, to penalize large structures, we look at $P_v = S_v - S_v' - \\lambda |T_v|$, where $|T_v|$ is the size of the subtree of $v$, and $\\lambda$ is a **pruning factor**. Larger $\\lambda$ results in more pruning. *(line 176)*\n",
        "* If $P_v < 0$, we decide that this subtree must be pruned. Intuitively, we prune when *pay more because of the size of the subtree than we gain because of variance reduction*. *(line 178 - 183)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV65TamQPGZY"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self):\n",
        "        self.predicted_value = None\n",
        "        self.decision_feature = None\n",
        "        self.decision_value = None\n",
        "        self.left_node = None\n",
        "        self.right_node = None\n",
        "\n",
        "class DecisionTree:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.root_node = None\n",
        "        self.min_coef_of_var = None\n",
        "        self.reductions = None\n",
        "\n",
        "    def fit(self, X, Y, min_coef_of_var = 0, pruning_factor = 0):\n",
        "        self.min_coef_of_var = min_coef_of_var\n",
        "        self.root_node = self.build(X, Y)\n",
        "        self.prune_decision_tree(X, Y, self.root_node, pruning_factor)\n",
        "\n",
        "        self.reductions = np.zeros(len(X[0]))\n",
        "        self.reductions = self.calculate_reductions(X, Y, self.root_node, self.reductions)\n",
        "\n",
        "    def get_decision_value_for_feature(self, X, Y, decision_feature):\n",
        "        \n",
        "        Z = np.empty((0, 2), float)\n",
        "        for i in range(len(Y)):\n",
        "            Z = np.append(Z, [[X[i, decision_feature], Y[i]]], axis = 0)\n",
        "\n",
        "        Z = np.sort(Z, axis = 0)\n",
        "\n",
        "        left_cardinality = 0\n",
        "        right_cardinality = 0\n",
        "        left_sum = 0\n",
        "        right_sum = 0\n",
        "        left_sum_of_squares = 0\n",
        "        right_sum_of_squares = 0\n",
        "\n",
        "        for i in range(len(Z)):\n",
        "            right_cardinality += 1\n",
        "            right_sum += Z[i, 1]\n",
        "            right_sum_of_squares += Z[i, 1] ** 2\n",
        "\n",
        "        current_best_impurity = right_sum_of_squares\n",
        "        current_best_decision_value = Z[0, 0] - 1\n",
        "\n",
        "        for i in range(len(Z)):\n",
        "            left_cardinality += 1\n",
        "            left_sum += Z[i, 1]\n",
        "            left_sum_of_squares += Z[i, 1] ** 2\n",
        "\n",
        "            right_cardinality -= 1\n",
        "            right_sum -= Z[i, 1]\n",
        "            right_sum_of_squares -= Z[i, 1] ** 2\n",
        "\n",
        "            impurity = left_sum_of_squares - left_sum * left_sum / left_cardinality\n",
        "            if (right_cardinality != 0):\n",
        "                impurity += right_sum_of_squares - right_sum * right_sum / right_cardinality\n",
        "            \n",
        "            if (impurity < current_best_impurity):\n",
        "                current_best_impurity = impurity\n",
        "                current_best_decision_value = Z[i, 0]\n",
        "\n",
        "        return current_best_decision_value\n",
        "\n",
        "    def get_impurity(self, X, Y, decision_feature, decision_value_for_feature):\n",
        "        left_cardinality = 0\n",
        "        right_cardinality = 0\n",
        "        left_sum = 0\n",
        "        right_sum = 0\n",
        "\n",
        "        for i in range(len(Y)):\n",
        "            if (X[i, decision_feature] <= decision_value_for_feature):\n",
        "                left_cardinality += 1\n",
        "                left_sum += Y[i]\n",
        "            else:\n",
        "                right_cardinality += 1\n",
        "                right_sum += Y[i]\n",
        "\n",
        "        if (left_cardinality != 0):\n",
        "            left_mean = left_sum / left_cardinality\n",
        "        else:\n",
        "            left_mean = 0\n",
        "\n",
        "        if (right_cardinality != 0):\n",
        "            right_mean = right_sum / right_cardinality\n",
        "        else:\n",
        "            right_mean = 0\n",
        "\n",
        "        impurity = 0\n",
        "        for i in range(len(Y)):\n",
        "            if (X[i, decision_feature] <= decision_value_for_feature):\n",
        "                impurity += (Y[i] - left_mean) ** 2\n",
        "            else:\n",
        "                impurity += (Y[i] - right_mean) ** 2\n",
        "\n",
        "        return impurity\n",
        "\n",
        "    def divide_data(self, X, Y, decision_feature, decision_value):\n",
        "        number_of_features = len(X[0])\n",
        "        X_left = np.empty((0, number_of_features), float)\n",
        "        X_right = np.empty((0, number_of_features), float)\n",
        "        Y_left = np.empty(0, float)\n",
        "        Y_right = np.empty(0, float)\n",
        "\n",
        "        for i in range(len(X)):\n",
        "            if (X[i, decision_feature] <= decision_value):\n",
        "                X_left = np.append(X_left, [X[i]], axis = 0)\n",
        "                Y_left = np.append(Y_left, [Y[i]], axis = 0)\n",
        "            else:\n",
        "                X_right = np.append(X_right, [X[i]], axis = 0)\n",
        "                Y_right = np.append(Y_right, [Y[i]], axis = 0)\n",
        "\n",
        "        return X_left, Y_left, X_right, Y_right\n",
        "\n",
        "    def build(self, X, Y):\n",
        "        if (len(X) == 0):\n",
        "            return\n",
        "\n",
        "        root_node = TreeNode()\n",
        "        node_mean = np.mean(Y)\n",
        "        root_node.predicted_value = node_mean\n",
        "        node_deviation = np.std(Y)\n",
        "\n",
        "        # Do not split\n",
        "        if (node_deviation / node_mean < self.min_coef_of_var):\n",
        "            root_node.decision_value = 0\n",
        "            root_node.decision_feature = 0\n",
        "            return\n",
        "\n",
        "        if (np.amin(Y) == np.amax(Y)):\n",
        "            root_node.decision_value = 0\n",
        "            root_node.decision_feature = 0\n",
        "            return root_node\n",
        "\n",
        "        number_of_features = len(X[0])\n",
        "        current_best_feature = -1\n",
        "        current_best_decision_value = -1\n",
        "        current_best_impurity = -1\n",
        "\n",
        "        for i in range(number_of_features):\n",
        "            decision_value_for_feature = self.get_decision_value_for_feature(X, Y, i)\n",
        "            impurity_of_decision_value = self.get_impurity(X, Y, i, decision_value_for_feature)\n",
        "\n",
        "            if (current_best_feature == -1 or impurity_of_decision_value < current_best_impurity):\n",
        "                current_best_feature = i\n",
        "                current_best_decision_value = decision_value_for_feature\n",
        "                current_best_impurity = impurity_of_decision_value\n",
        "\n",
        "        root_node.decision_feature = current_best_feature\n",
        "        root_node.decision_value = current_best_decision_value\n",
        "\n",
        "        X_left, Y_left, X_right, Y_right = self.divide_data(X, Y, root_node.decision_feature, root_node.decision_value)\n",
        "\n",
        "        if (len(Y_left) == 0 or len(Y_right) == 0):\n",
        "            return root_node\n",
        "\n",
        "        root_node.left_node = self.build(X_left, Y_left)\n",
        "        root_node.right_node = self.build(X_right, Y_right)\n",
        "        return root_node\n",
        "\n",
        "    def prune_decision_tree(self, X, Y, root_node, pruning_factor):\n",
        "        if (root_node == None):\n",
        "          return 0\n",
        "\n",
        "        X_left, Y_left, X_right, Y_right = self.divide_data(X, Y, root_node.decision_feature, root_node.decision_value)\n",
        "        left_tree_size = self.prune_decision_tree(X_left, Y_left, root_node.left_node, pruning_factor)\n",
        "        right_tree_size = self.prune_decision_tree(X_right, Y_right, root_node.right_node, pruning_factor)\n",
        "        tree_size = 1 + left_tree_size + right_tree_size\n",
        "\n",
        "        impurity = 0\n",
        "        for y in Y:\n",
        "            impurity += (y - root_node.predicted_value) ** 2\n",
        "\n",
        "        divided_impurity = self.get_impurity(X, Y, root_node.decision_feature, root_node.decision_value)\n",
        "\n",
        "        pruning_value = impurity / len(Y) - divided_impurity / len(Y) - pruning_factor * tree_size\n",
        "\n",
        "        if (pruning_value < 0):\n",
        "            root_node.left_node = None\n",
        "            root_node.right_node = None\n",
        "            return 1\n",
        "        else:\n",
        "            return tree_size\n",
        "    \n",
        "    def calculate_reductions(self, X, Y, root_node, reductions):\n",
        "        if (root_node == None):\n",
        "            return reductions\n",
        "\n",
        "        impurity = 0\n",
        "        for y in Y:\n",
        "            impurity += (y - root_node.predicted_value) ** 2\n",
        "\n",
        "        for i in range(len(X[0])):\n",
        "            decision_value_for_feature = self.get_decision_value_for_feature(X, Y, i)\n",
        "            impurity_of_decision_value = self.get_impurity(X, Y, i, decision_value_for_feature)\n",
        "\n",
        "            reductions[i] += impurity - impurity_of_decision_value\n",
        "        \n",
        "        X_left, Y_left, X_right, Y_right = self.divide_data(X, Y, root_node.decision_feature, root_node.decision_value)\n",
        "        reductions = self.calculate_reductions(X_left, Y_left, root_node.left_node, reductions)\n",
        "        reductions = self.calculate_reductions(X_right, Y_right, root_node.right_node, reductions)\n",
        "\n",
        "        return reductions\n",
        "    \n",
        "    def get_reductions(self):\n",
        "        return self.reductions\n",
        "\n",
        "    def predict_single(self, X):\n",
        "        current_node = self.root_node\n",
        "        while (True):\n",
        "            if (X[current_node.decision_feature] <= current_node.decision_value):\n",
        "                if (current_node.left_node != None):\n",
        "                    current_node = current_node.left_node\n",
        "                else:\n",
        "                    return current_node.predicted_value\n",
        "            else:\n",
        "                if (current_node.right_node != None):\n",
        "                    current_node = current_node.right_node\n",
        "                else:\n",
        "                    return current_node.predicted_value\n",
        "\n",
        "    def predict(self, X):\n",
        "        Y = np.empty(0, float)\n",
        "        for x in X:\n",
        "            Y = np.append(Y, [self.predict_single(x)], axis = 0)\n",
        "\n",
        "        return Y\n",
        "\n",
        "decision_tree = DecisionTree()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJnQKidglmyF"
      },
      "source": [
        "We will use Boston housing data from ```sklearn.datasets``` for testing our algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z63Hg3prQLZS"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "data = load_boston(return_X_y=False)\n",
        "X = data.data\n",
        "y = data.target\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB_S17NXmJco"
      },
      "source": [
        "We fit our decision tree to the training data, and check our performance on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF9wssGhQTBu",
        "outputId": "91805836-ddc9-493c-f313-2e2f3dc73476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decision_tree.fit(X_train, y_train, min_coef_of_var = 0.025, pruning_factor = 0)\n",
        "y_test_pred = decision_tree.predict(X_test)\n",
        "metrics.r2_score(y_test, y_test_pred)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7114290465682265"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtTdwSdrm6rZ"
      },
      "source": [
        "We see that the score of our predictions after training the data on training set is (~71%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqZwFgevM9oF",
        "outputId": "b183a63b-b3d5-4013-8655-da49f504f932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "decision_tree.get_reductions()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([13399.20701222,  5186.7937058 ,  6077.93066069,  3654.66842837,\n",
              "       15268.52069709, 20767.48739927, 10509.61121374,  9736.48016117,\n",
              "        5424.56393685,  6267.04097296,  9541.52648792,  3903.72194251,\n",
              "       21012.15264691])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4Zs2-2SlM9_",
        "outputId": "0445f264-443d-44c8-b743-03b61c77a4f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decision_tree.fit(X_train, y_train, min_coef_of_var = 0.025, pruning_factor = 0.4)\n",
        "y_test_pred = decision_tree.predict(X_test)\n",
        "metrics.r2_score(y_test, y_test_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7428687746808185"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzcvbpMAnXIs"
      },
      "source": [
        "The score is significantly improved (from ~71% to ~74%) if we use pruning (with pruning factor = 0.4). Next we check how scikit-learn fares on the same test:train split.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0_BiqDpRDfD",
        "outputId": "ffb397f6-8cad-4b90-d48d-53ca488e6153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor()\n",
        "dt.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJdyftNvi82D",
        "outputId": "3c770dbd-84ba-422f-dbb7-2673f2f8aa7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_test_pred_dt = dt.predict(X_test)\n",
        "metrics.r2_score(y_test, y_test_pred_dt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8055224772577356"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUlJuDhLp5rM"
      },
      "source": [
        "As we see, scikit-learn's implementation gives a score of (~80%). Their implementation is more advanced than our implementation which uses nothing but the absolute basics of decision trees. \n",
        "\n",
        "Now, we investigate how our algorithm performed using a graphical approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYAU8oFIi_SA",
        "outputId": "d908eb24-f672-4ee4-ddfe-13a7a25a436b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(y_test, y_test, color = \"green\")\n",
        "plt.xlabel(\"actual\")\n",
        "plt.ylabel(\"predicted\")\n",
        "plt.scatter(y_test, y_test_pred)\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtcfaQviqCdx"
      },
      "source": [
        "We plot the predicted value of price of house on the y-axis and actual value on the x-axis. Ideally all the points should lie on the y = x line. We observe that most of the points lie close to the y = x line. We also observe that there are some horizonal lines in the plot implying same predicted value for a range of actual values. This is because we implemented a basic decision tree. We will see later that these horizontal lines disappear when we use more advanced techniques like the random-forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7yFrwtj16gG"
      },
      "source": [
        "## Random Forests\n",
        "\n",
        "Now we take a look at simple ensemble methods using decision trees. The most famous of these is the classical Random Forest, using Bagging / Random Subspace projection methods.\n",
        "\n",
        "But, what is a _random forest_ ? A random forest is simply a collection of trees trained on random subsets of the data (either resampled data points and/or subset of attributes). Here is a picture of a 2-tree random forest:\n",
        "\n",
        "![](https://github.com/mbrc12/random-images/raw/master/two-tree-random-forest.png)\n",
        "\n",
        "Another question is, how do we actually _use_ this structure to generate predictions. In the simplest form, this prediction is simply done by averaging the answers from the trees. Note that we use averaging here because the problem we are solving is **regression** as opposed to **classification**.\n",
        "\n",
        "Of course, this is not the only method that can be used. Other methods involve boosting trees, which train trees on the residual from other trees, but we do not delve into this matter here.\n",
        "\n",
        "### Construction\n",
        "\n",
        "Now let us take a look at how we construct these forests. The code for the algorithm described here is included below.\n",
        "\n",
        "Firstly, note that we take the algorithm for Decision Trees as a black-box (in our code, we use the Decision Trees above). Given a training dataset, we form a fixed (hyperparameter) number of trees (_lines 21-30 below_). For each tree, we choose a resampled (with replacement) subset of the datapoints (_line 23_), _and_ a subset of the attributes (_line 24_). The size of these subsets are controlled by the hyperparameters. Then we train the tree on this data (_line 28_).\n",
        "\n",
        "The first technique of using resampled data is very similar to the bootstrap method, and can also be used for **constructing <u>confidence intervals</u> for the answers**. It is also referred to as **bagging / bootstrap-aggregating** in the ensemble learning literature.\n",
        "\n",
        "The second technique of choosing a random subset of the attributes is part of a generic technique for dealing with high-dimensional data, called **random subspace projection**. The name is so because choosing a particular set of attributes/coordinates is equivalent to projecting the high-dimensional datapoints to smaller subspaces.\n",
        "\n",
        "**Note:** In our use-case we do not use random-subspace projection, because we have only a few attributes (~15), in which the projection method doesn't help much.\n",
        "\n",
        "### Prediction\n",
        "\n",
        "The prediction is implemented in lines _34-43_. It is a simple algorithm in our case: just average the answers from all the trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrfUQZX0nm5H"
      },
      "source": [
        "from numpy.random import choice, seed\n",
        "\n",
        "class RandomForest:\n",
        "\n",
        "    trees = []\n",
        "    importance = None\n",
        "\n",
        "    def __init__(self):\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y, num_trees = 1, frac_samples = None, frac_attrs = None):\n",
        "        seed(0)\n",
        "        \n",
        "        N, total_attributes = X.shape \n",
        "        self.importance = np.zeros(total_attributes)\n",
        "\n",
        "        if frac_samples == None:\n",
        "            frac_samples = 1.0\n",
        "\n",
        "        if frac_attrs == None:\n",
        "            frac_attrs = 1.0\n",
        "\n",
        "        for i in range(num_trees):\n",
        "            new_tree = DecisionTree()\n",
        "            sample = choice(range(N), size = int(N * frac_samples))\n",
        "            attrs = choice(range(total_attributes), size = int(total_attributes * frac_attrs))\n",
        "            #print(attrs, end=\", \")\n",
        "            dataset = X[sample, :]   #[:, attrs]\n",
        "            ys = y[sample]\n",
        "            new_tree.fit(dataset, ys, min_coef_of_var = 0.025, pruning_factor = 0)\n",
        "\n",
        "            self.importance += new_tree.get_reductions()\n",
        "\n",
        "            self.trees.append((new_tree, attrs))\n",
        "\n",
        "        self.importance /= num_trees\n",
        "\n",
        "    def predict(self, X):\n",
        "        \n",
        "        N, _ = X.shape \n",
        "\n",
        "        answer = np.zeros(N)\n",
        "\n",
        "        for (t, attrs) in self.trees:\n",
        "            answer += t.predict(X) # [:, attrs])\n",
        "        \n",
        "        return answer / len(self.trees)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLVOgwHt65FA"
      },
      "source": [
        "Now, we train the random forest on our training data, using a 0.8 fraction of (resampled) data-points. However, we choose not to use to the projection method, for reasons described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiZjRa6Htj0k"
      },
      "source": [
        "rf = RandomForest()\n",
        "rf.fit(X_train, y_train, num_trees = 30, frac_samples = 0.8, frac_attrs = 1.0)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz4sagAe7g69"
      },
      "source": [
        "And then we test it on our test-data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thF_c4Uetrsj",
        "outputId": "1f228a0d-691a-41fa-b4bf-3ba39f9826f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test_rf = rf.predict(X_test)\n",
        "y_test_rf\n",
        "metrics.r2_score(y_test, y_test_rf)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8545010166573305"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvMNQZzf7l6-"
      },
      "source": [
        "We see a really good improvement from the raw Decision Tree score (~72% to ~85%). Let us also compare it to the scikit-learn implementation (which is more advanced than ours):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o_jN3MpuEc4",
        "outputId": "74803558-c867-4520-8347-be4f62c0da9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfsk = RandomForestRegressor(bootstrap=True)\n",
        "rfsk.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
              "                      random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YADeDtpRwzKT",
        "outputId": "cbac7ead-ea66-4b92-839f-abc9ce9d2012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test_rfsk = rfsk.predict(X_test)\n",
        "metrics.r2_score(y_test, y_test_rfsk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9099455853887437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2-61CTh76uO"
      },
      "source": [
        "We see that scikit-learn manages an accuracy of ~91%. Our 85% is not that worse, given that our implementation is very bare-bones, and mostly proof-of-concept. This shows the power of the very idea of random forests (and in general, ensemble learning). \n",
        "\n",
        "Finally, we plot our predictions as opposed to the ground truth (x = y). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ42jxDD3dLR",
        "outputId": "2008f936-341a-436d-c2d7-e5a16a830e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(y_test, y_test, color = \"green\")\n",
        "plt.xlabel(\"actual\")\n",
        "plt.ylabel(\"predicted\")\n",
        "plt.scatter(y_test, y_test_rf)\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFzloE7U8W4J"
      },
      "source": [
        "We see that a large chunk of the predicted points is really close to the ground truths. Also, we observe no horizontal segments like in the case for Decision Trees. This is because of the averaging step in the Random Forests, and contributes immensely to the accuracy.\n",
        "\n",
        "## Variable Importance\n",
        "\n",
        "Now we take a look at the importance of variables. For this purpose, we compute the following quantity for each tree $T$ in the ensemble:\n",
        "\n",
        "$\\mathrm{VarReduction}_i = \\sum_{x \\in T} \\mathrm{Optimal\\ Variance\\ Reduction\\ in\\ node\\ } x \\mathrm{\\ by\\ attribute\\ } i$\n",
        "\n",
        "and then denote \n",
        "\n",
        "$\\mathrm{Importance}_i = \\mathrm{Average\\ of\\ VarReduction}_i$\n",
        "\n",
        "This quantity will be the importance of attribute $i$. We plot these quantities in a relative sense.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxnUDubGHjQx",
        "outputId": "77a82f6a-4066-4221-a33f-54c3e1e0123c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "importance = rf.importance\n",
        "relative_importance = importance / np.sum(importance)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(data.feature_names, relative_importance)\n",
        "plt.ylabel('Relative Importance')\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKCn5GQgQ_3b"
      },
      "source": [
        "The following are the meanings of the attributes:\n",
        "\n",
        "    CRIM - per capita crime rate by town\n",
        "    ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "    INDUS - proportion of non-retail business acres per town.\n",
        "    CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
        "    NOX - nitric oxides concentration (parts per 10 million)\n",
        "    RM - average number of rooms per dwelling\n",
        "    AGE - proportion of owner-occupied units built prior to 1940\n",
        "    DIS - weighted distances to five Boston employment centres\n",
        "    RAD - index of accessibility to radial highways\n",
        "    TAX - full-value property-tax rate per $10,000\n",
        "    PTRATIO - pupil-teacher ratio by town\n",
        "    B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "    LSTAT - % lower status of the population\n",
        "    MEDV - Median value of owner-occupied homes in $1000's\n",
        "\n",
        "\n",
        "So we can see that the most important attribute is RM, which is the average number of rooms. This makes sense, because the number of rooms is an important factor. The 2nd most important factor is LSTAT, which is the percentage of the population which is of lower status. This can be interpreted to mean that the price is lower in underdeveloped regions. And the 3rd most important factor is crime rate (CRIM), which too makes a lot of sense. On the other end, the CHAS variable is about the Charles River. It doesn't make too much of an impact on the price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PgMAamUQKn4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}